{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je2v0-P-UFax",
        "colab_type": "text"
      },
      "source": [
        "#Задание 6: Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwRyqd04tgXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 -qq install torch\n",
        "!pip3 -qq install bokeh\n",
        "!pip3 -qq install gensim\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixnQeeFSP5ls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "413606a0-b010-4a2c-f7cc-d44b4edb904a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jan 18 03:10:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u62wtAf9RIH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l1FJJacwKye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3e6KllRRmHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f5f38dcb-8818-45c6-9859-e7649140a9e3"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps6cfoQYSAxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "808be6fe-9290-4fab-8add-1774022af11f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilepY7wvSCKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "b9059f78-440c-4841-958a-8a8c9b65d838"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw0kpisfSKp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bec1f1fc-f28e-4e52-a944-5ead9548aa51"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itue9hFfSsyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47ff79ab-f9d3-4290-ea98-0a438233cb0f"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'ADP', 'CONJ', 'ADJ', 'VERB', 'NOUN', 'ADV', 'DET', 'PRON', 'X', 'PRT', 'NUM', '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODDncao9SzPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "2692f166-3182-4b7b-f05f-5795cc365877"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJkmS1CFDmiRJUocMaZIkSR36/wBdhXVcpa0oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtQ3jZZULN8",
        "colab_type": "text"
      },
      "source": [
        "#Бейзлайн"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ-_PhiXTXM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f3b2215-ce3e-4965-bbf2-d4461afc92f9"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnBFq1cTUvgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f502e16-67d8-41c8-ee1b-4ee8894f6dad"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1uREPtoVB-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d34b781-d388-4414-b4ff-1feeea69eee1"
      },
      "source": [
        "\n",
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5VJa3RjVaMF",
        "colab_type": "text"
      },
      "source": [
        "#Увеличиваем контекст с рекуррентными сетями"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HKlX1lYVQmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_jpO7bWVyKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsrd2_bQV10q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9fea503-6f3d-46cc-db0a-0edcde59d49b"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTfGtzq0WKsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66HtQUhJW6GA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39e57977-6623-48cc-933e-d1daaa38d508"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "model.type(torch.cuda.FloatTensor)\n",
        "model = model.to(device)\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "x_batch_gpu = X_batch.to(device)\n",
        "y_batch_gpu = y_batch.to(device)\n",
        "logits = model(x_batch_gpu)\n",
        "prediction = torch.argmax(logits, dim=-1)\n",
        "mask = (y_batch_gpu != 0).float()\n",
        "correct_samples  = ((prediction == y_batch_gpu).float() * mask).sum().item()\n",
        "total_samples = mask.sum().item()\n",
        "\n",
        "accuracy = correct_samples / total_samples\n",
        "print(accuracy)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09782608695652174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5FymwOFq009",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fce5ce07-bfbf-482d-96d2-81ad5cf8e5f9"
      },
      "source": [
        "loss = nn.CrossEntropyLoss(ignore_index=0).type(torch.cuda.FloatTensor)\n",
        "loss_value = 0\n",
        "for ind, row in enumerate(logits):\n",
        "    loss_value += loss(row, y_batch_gpu[ind])\n",
        "loss_value = loss_value.item()\n",
        "print(loss_value)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82.26701354980469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDJxoI5Atwgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = 0\n",
        "                for ind, row in enumerate(logits):\n",
        "                    loss += criterion(row, y_batch[ind])\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                prediction = torch.argmax(logits, dim=-1)\n",
        "                mask = (y_batch != 0).float()\n",
        "                cur_correct_count, cur_sum_count = ((prediction == y_batch).float() * mask).sum().item(), mask.sum().item()\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
        "\n",
        "\n",
        "def compute_accuracy(model, dataset, batch_size=64):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(dataset, batch_size)):\n",
        "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        \n",
        "        prediction = torch.argmax(logits, dim=-1)\n",
        "        mask = (y_batch != 0).float()\n",
        "        correct += ((prediction == y_batch).float() * mask).sum().item()\n",
        "        total += mask.sum().item()        \n",
        "        \n",
        "    return float(correct)/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiHWlF3ftwni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d027ffa3-35f9-4fab-d920-e851d4a26745"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "model.type(torch.cuda.FloatTensor)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).type(torch.cuda.FloatTensor)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-3)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 55.89747, Accuracy = 70.54%: 100%|██████████| 572/572 [00:10<00:00, 53.38it/s]\n",
            "[1 / 50]   Val: Loss = 55.38466, Accuracy = 80.94%: 100%|██████████| 13/13 [00:00<00:00, 52.08it/s]\n",
            "[2 / 50] Train: Loss = 25.47732, Accuracy = 85.50%: 100%|██████████| 572/572 [00:10<00:00, 54.30it/s]\n",
            "[2 / 50]   Val: Loss = 39.13791, Accuracy = 86.52%: 100%|██████████| 13/13 [00:00<00:00, 78.06it/s]\n",
            "[3 / 50] Train: Loss = 16.95612, Accuracy = 90.26%: 100%|██████████| 572/572 [00:10<00:00, 54.95it/s]\n",
            "[3 / 50]   Val: Loss = 32.91416, Accuracy = 89.92%: 100%|██████████| 13/13 [00:00<00:00, 77.28it/s]\n",
            "[4 / 50] Train: Loss = 11.57130, Accuracy = 93.06%: 100%|██████████| 572/572 [00:10<00:00, 53.86it/s]\n",
            "[4 / 50]   Val: Loss = 28.85817, Accuracy = 91.89%: 100%|██████████| 13/13 [00:00<00:00, 76.04it/s]\n",
            "[5 / 50] Train: Loss = 8.42459, Accuracy = 94.55%: 100%|██████████| 572/572 [00:10<00:00, 59.56it/s] \n",
            "[5 / 50]   Val: Loss = 27.88556, Accuracy = 92.67%: 100%|██████████| 13/13 [00:00<00:00, 74.71it/s]\n",
            "[6 / 50] Train: Loss = 6.29696, Accuracy = 95.46%: 100%|██████████| 572/572 [00:10<00:00, 55.60it/s]\n",
            "[6 / 50]   Val: Loss = 27.37339, Accuracy = 93.01%: 100%|██████████| 13/13 [00:00<00:00, 74.35it/s]\n",
            "[7 / 50] Train: Loss = 5.27161, Accuracy = 96.02%: 100%|██████████| 572/572 [00:11<00:00, 47.90it/s]\n",
            "[7 / 50]   Val: Loss = 23.17675, Accuracy = 93.17%: 100%|██████████| 13/13 [00:00<00:00, 59.99it/s]\n",
            "[8 / 50] Train: Loss = 4.67853, Accuracy = 96.35%: 100%|██████████| 572/572 [00:11<00:00, 51.74it/s]\n",
            "[8 / 50]   Val: Loss = 24.38637, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 73.62it/s]\n",
            "[9 / 50] Train: Loss = 4.30299, Accuracy = 96.57%: 100%|██████████| 572/572 [00:11<00:00, 51.47it/s]\n",
            "[9 / 50]   Val: Loss = 24.33002, Accuracy = 93.37%: 100%|██████████| 13/13 [00:00<00:00, 63.91it/s]\n",
            "[10 / 50] Train: Loss = 3.93263, Accuracy = 96.80%: 100%|██████████| 572/572 [00:10<00:00, 53.25it/s]\n",
            "[10 / 50]   Val: Loss = 24.47383, Accuracy = 93.50%: 100%|██████████| 13/13 [00:00<00:00, 75.83it/s]\n",
            "[11 / 50] Train: Loss = 3.75837, Accuracy = 96.89%: 100%|██████████| 572/572 [00:11<00:00, 49.90it/s]\n",
            "[11 / 50]   Val: Loss = 23.59305, Accuracy = 93.58%: 100%|██████████| 13/13 [00:00<00:00, 63.68it/s]\n",
            "[12 / 50] Train: Loss = 3.50152, Accuracy = 97.04%: 100%|██████████| 572/572 [00:11<00:00, 49.99it/s]\n",
            "[12 / 50]   Val: Loss = 23.02903, Accuracy = 93.65%: 100%|██████████| 13/13 [00:00<00:00, 61.71it/s]\n",
            "[13 / 50] Train: Loss = 3.28139, Accuracy = 97.15%: 100%|██████████| 572/572 [00:10<00:00, 52.14it/s]\n",
            "[13 / 50]   Val: Loss = 24.26626, Accuracy = 93.51%: 100%|██████████| 13/13 [00:00<00:00, 72.77it/s]\n",
            "[14 / 50] Train: Loss = 3.26951, Accuracy = 97.16%: 100%|██████████| 572/572 [00:10<00:00, 53.24it/s]\n",
            "[14 / 50]   Val: Loss = 21.82532, Accuracy = 93.57%: 100%|██████████| 13/13 [00:00<00:00, 62.15it/s]\n",
            "[15 / 50] Train: Loss = 2.94583, Accuracy = 97.32%: 100%|██████████| 572/572 [00:10<00:00, 54.72it/s]\n",
            "[15 / 50]   Val: Loss = 25.47706, Accuracy = 93.60%: 100%|██████████| 13/13 [00:00<00:00, 67.05it/s]\n",
            "[16 / 50] Train: Loss = 2.85096, Accuracy = 97.40%: 100%|██████████| 572/572 [00:10<00:00, 53.82it/s]\n",
            "[16 / 50]   Val: Loss = 26.76987, Accuracy = 93.47%: 100%|██████████| 13/13 [00:00<00:00, 72.34it/s]\n",
            "[17 / 50] Train: Loss = 2.68672, Accuracy = 97.48%: 100%|██████████| 572/572 [00:11<00:00, 46.14it/s]\n",
            "[17 / 50]   Val: Loss = 27.69595, Accuracy = 93.64%: 100%|██████████| 13/13 [00:00<00:00, 63.36it/s]\n",
            "[18 / 50] Train: Loss = 2.65622, Accuracy = 97.50%: 100%|██████████| 572/572 [00:11<00:00, 55.70it/s]\n",
            "[18 / 50]   Val: Loss = 25.24916, Accuracy = 93.67%: 100%|██████████| 13/13 [00:00<00:00, 57.27it/s]\n",
            "[19 / 50] Train: Loss = 2.65058, Accuracy = 97.54%: 100%|██████████| 572/572 [00:11<00:00, 49.05it/s]\n",
            "[19 / 50]   Val: Loss = 25.91235, Accuracy = 93.65%: 100%|██████████| 13/13 [00:00<00:00, 70.13it/s]\n",
            "[20 / 50] Train: Loss = 2.44449, Accuracy = 97.66%: 100%|██████████| 572/572 [00:11<00:00, 49.72it/s]\n",
            "[20 / 50]   Val: Loss = 24.14428, Accuracy = 93.60%: 100%|██████████| 13/13 [00:00<00:00, 71.45it/s]\n",
            "[21 / 50] Train: Loss = 2.25717, Accuracy = 97.75%: 100%|██████████| 572/572 [00:11<00:00, 50.90it/s]\n",
            "[21 / 50]   Val: Loss = 25.46666, Accuracy = 93.60%: 100%|██████████| 13/13 [00:00<00:00, 62.66it/s]\n",
            "[22 / 50] Train: Loss = 2.36256, Accuracy = 97.70%: 100%|██████████| 572/572 [00:11<00:00, 49.04it/s]\n",
            "[22 / 50]   Val: Loss = 25.04570, Accuracy = 93.69%: 100%|██████████| 13/13 [00:00<00:00, 61.86it/s]\n",
            "[23 / 50] Train: Loss = 2.14070, Accuracy = 97.84%: 100%|██████████| 572/572 [00:12<00:00, 47.10it/s]\n",
            "[23 / 50]   Val: Loss = 25.34962, Accuracy = 93.71%: 100%|██████████| 13/13 [00:00<00:00, 65.07it/s]\n",
            "[24 / 50] Train: Loss = 2.08475, Accuracy = 97.89%: 100%|██████████| 572/572 [00:11<00:00, 48.46it/s]\n",
            "[24 / 50]   Val: Loss = 27.54652, Accuracy = 93.73%: 100%|██████████| 13/13 [00:00<00:00, 58.61it/s]\n",
            "[25 / 50] Train: Loss = 1.97803, Accuracy = 98.00%: 100%|██████████| 572/572 [00:11<00:00, 48.87it/s]\n",
            "[25 / 50]   Val: Loss = 24.69616, Accuracy = 93.80%: 100%|██████████| 13/13 [00:00<00:00, 63.05it/s]\n",
            "[26 / 50] Train: Loss = 2.03352, Accuracy = 97.95%: 100%|██████████| 572/572 [00:11<00:00, 48.01it/s]\n",
            "[26 / 50]   Val: Loss = 26.90827, Accuracy = 93.73%: 100%|██████████| 13/13 [00:00<00:00, 54.35it/s]\n",
            "[27 / 50] Train: Loss = 1.99992, Accuracy = 98.00%: 100%|██████████| 572/572 [00:11<00:00, 48.46it/s]\n",
            "[27 / 50]   Val: Loss = 29.45265, Accuracy = 93.64%: 100%|██████████| 13/13 [00:00<00:00, 64.09it/s]\n",
            "[28 / 50] Train: Loss = 1.82391, Accuracy = 98.09%: 100%|██████████| 572/572 [00:12<00:00, 45.67it/s]\n",
            "[28 / 50]   Val: Loss = 26.88669, Accuracy = 93.69%: 100%|██████████| 13/13 [00:00<00:00, 67.93it/s]\n",
            "[29 / 50] Train: Loss = 1.85579, Accuracy = 98.09%: 100%|██████████| 572/572 [00:12<00:00, 47.58it/s]\n",
            "[29 / 50]   Val: Loss = 27.46175, Accuracy = 93.60%: 100%|██████████| 13/13 [00:00<00:00, 59.60it/s]\n",
            "[30 / 50] Train: Loss = 1.76965, Accuracy = 98.16%: 100%|██████████| 572/572 [00:11<00:00, 48.48it/s]\n",
            "[30 / 50]   Val: Loss = 22.75542, Accuracy = 93.93%: 100%|██████████| 13/13 [00:00<00:00, 70.22it/s]\n",
            "[31 / 50] Train: Loss = 1.80676, Accuracy = 98.12%: 100%|██████████| 572/572 [00:11<00:00, 51.96it/s]\n",
            "[31 / 50]   Val: Loss = 29.29200, Accuracy = 93.62%: 100%|██████████| 13/13 [00:00<00:00, 65.22it/s]\n",
            "[32 / 50] Train: Loss = 1.96625, Accuracy = 98.07%: 100%|██████████| 572/572 [00:11<00:00, 51.59it/s]\n",
            "[32 / 50]   Val: Loss = 27.91610, Accuracy = 93.80%: 100%|██████████| 13/13 [00:00<00:00, 65.86it/s]\n",
            "[33 / 50] Train: Loss = 1.61583, Accuracy = 98.26%: 100%|██████████| 572/572 [00:11<00:00, 50.55it/s]\n",
            "[33 / 50]   Val: Loss = 28.34385, Accuracy = 93.76%: 100%|██████████| 13/13 [00:00<00:00, 67.81it/s]\n",
            "[34 / 50] Train: Loss = 1.61805, Accuracy = 98.30%: 100%|██████████| 572/572 [00:11<00:00, 51.78it/s]\n",
            "[34 / 50]   Val: Loss = 27.38404, Accuracy = 93.82%: 100%|██████████| 13/13 [00:00<00:00, 57.60it/s]\n",
            "[35 / 50] Train: Loss = 1.66653, Accuracy = 98.27%: 100%|██████████| 572/572 [00:11<00:00, 50.19it/s]\n",
            "[35 / 50]   Val: Loss = 28.72570, Accuracy = 93.62%: 100%|██████████| 13/13 [00:00<00:00, 69.69it/s]\n",
            "[36 / 50] Train: Loss = 1.60760, Accuracy = 98.31%: 100%|██████████| 572/572 [00:11<00:00, 51.55it/s]\n",
            "[36 / 50]   Val: Loss = 27.38871, Accuracy = 93.78%: 100%|██████████| 13/13 [00:00<00:00, 58.37it/s]\n",
            "[37 / 50] Train: Loss = 1.58184, Accuracy = 98.34%: 100%|██████████| 572/572 [00:12<00:00, 46.03it/s]\n",
            "[37 / 50]   Val: Loss = 28.25562, Accuracy = 93.71%: 100%|██████████| 13/13 [00:00<00:00, 58.22it/s]\n",
            "[38 / 50] Train: Loss = 1.78765, Accuracy = 98.21%: 100%|██████████| 572/572 [00:12<00:00, 46.15it/s]\n",
            "[38 / 50]   Val: Loss = 26.30493, Accuracy = 93.67%: 100%|██████████| 13/13 [00:00<00:00, 59.24it/s]\n",
            "[39 / 50] Train: Loss = 1.60656, Accuracy = 98.32%: 100%|██████████| 572/572 [00:11<00:00, 50.50it/s]\n",
            "[39 / 50]   Val: Loss = 25.28111, Accuracy = 93.77%: 100%|██████████| 13/13 [00:00<00:00, 69.81it/s]\n",
            "[40 / 50] Train: Loss = 1.51653, Accuracy = 98.37%: 100%|██████████| 572/572 [00:12<00:00, 46.82it/s]\n",
            "[40 / 50]   Val: Loss = 27.77996, Accuracy = 93.78%: 100%|██████████| 13/13 [00:00<00:00, 60.43it/s]\n",
            "[41 / 50] Train: Loss = 1.37606, Accuracy = 98.49%: 100%|██████████| 572/572 [00:12<00:00, 45.22it/s]\n",
            "[41 / 50]   Val: Loss = 24.68468, Accuracy = 93.86%: 100%|██████████| 13/13 [00:00<00:00, 61.19it/s]\n",
            "[42 / 50] Train: Loss = 1.53526, Accuracy = 98.39%: 100%|██████████| 572/572 [00:12<00:00, 45.97it/s]\n",
            "[42 / 50]   Val: Loss = 27.56371, Accuracy = 93.83%: 100%|██████████| 13/13 [00:00<00:00, 49.36it/s]\n",
            "[43 / 50] Train: Loss = 1.65419, Accuracy = 98.34%: 100%|██████████| 572/572 [00:12<00:00, 46.13it/s]\n",
            "[43 / 50]   Val: Loss = 28.09973, Accuracy = 93.51%: 100%|██████████| 13/13 [00:00<00:00, 63.15it/s]\n",
            "[44 / 50] Train: Loss = 1.44167, Accuracy = 98.45%: 100%|██████████| 572/572 [00:12<00:00, 45.00it/s]\n",
            "[44 / 50]   Val: Loss = 25.72029, Accuracy = 93.70%: 100%|██████████| 13/13 [00:00<00:00, 63.03it/s]\n",
            "[45 / 50] Train: Loss = 1.44331, Accuracy = 98.47%: 100%|██████████| 572/572 [00:12<00:00, 48.29it/s]\n",
            "[45 / 50]   Val: Loss = 28.08502, Accuracy = 93.89%: 100%|██████████| 13/13 [00:00<00:00, 48.70it/s]\n",
            "[46 / 50] Train: Loss = 1.36932, Accuracy = 98.54%: 100%|██████████| 572/572 [00:12<00:00, 46.54it/s]\n",
            "[46 / 50]   Val: Loss = 26.74394, Accuracy = 93.72%: 100%|██████████| 13/13 [00:00<00:00, 58.65it/s]\n",
            "[47 / 50] Train: Loss = 1.48610, Accuracy = 98.46%: 100%|██████████| 572/572 [00:12<00:00, 44.86it/s]\n",
            "[47 / 50]   Val: Loss = 23.82841, Accuracy = 93.83%: 100%|██████████| 13/13 [00:00<00:00, 51.93it/s]\n",
            "[48 / 50] Train: Loss = 1.37275, Accuracy = 98.53%: 100%|██████████| 572/572 [00:12<00:00, 46.81it/s]\n",
            "[48 / 50]   Val: Loss = 24.73839, Accuracy = 93.79%: 100%|██████████| 13/13 [00:00<00:00, 60.26it/s]\n",
            "[49 / 50] Train: Loss = 1.31647, Accuracy = 98.57%: 100%|██████████| 572/572 [00:12<00:00, 45.66it/s]\n",
            "[49 / 50]   Val: Loss = 25.51662, Accuracy = 93.79%: 100%|██████████| 13/13 [00:00<00:00, 69.77it/s]\n",
            "[50 / 50] Train: Loss = 1.47271, Accuracy = 98.47%: 100%|██████████| 572/572 [00:11<00:00, 51.74it/s]\n",
            "[50 / 50]   Val: Loss = 27.52471, Accuracy = 93.60%: 100%|██████████| 13/13 [00:00<00:00, 65.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04wJrU9X53vZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e1bb956-b115-4626-9144-a696be63c7cc"
      },
      "source": [
        "accuracy =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {accuracy * 100} %')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 93.70741800329812 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saZYiDTM-JVV",
        "colab_type": "text"
      },
      "source": [
        "#Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVolKrhA9IyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPFIbDI_GMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "861a7aef-da72-4d6e-bc81-7b35112102ee"
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    lstm_layers_count=2,\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "model.type(torch.cuda.FloatTensor)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).type(torch.cuda.FloatTensor)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 10] Train: Loss = 42.71084, Accuracy = 76.69%: 100%|██████████| 572/572 [00:18<00:00, 31.61it/s]\n",
            "[1 / 10]   Val: Loss = 39.11693, Accuracy = 86.03%: 100%|██████████| 13/13 [00:00<00:00, 32.90it/s]\n",
            "[2 / 10] Train: Loss = 16.26652, Accuracy = 90.76%: 100%|██████████| 572/572 [00:17<00:00, 33.28it/s]\n",
            "[2 / 10]   Val: Loss = 30.68741, Accuracy = 90.55%: 100%|██████████| 13/13 [00:00<00:00, 31.99it/s]\n",
            "[3 / 10] Train: Loss = 9.54407, Accuracy = 94.35%: 100%|██████████| 572/572 [00:17<00:00, 32.24it/s]\n",
            "[3 / 10]   Val: Loss = 25.39520, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 31.78it/s]\n",
            "[4 / 10] Train: Loss = 5.75777, Accuracy = 96.20%: 100%|██████████| 572/572 [00:17<00:00, 32.43it/s]\n",
            "[4 / 10]   Val: Loss = 23.28302, Accuracy = 93.80%: 100%|██████████| 13/13 [00:00<00:00, 31.92it/s]\n",
            "[5 / 10] Train: Loss = 3.54692, Accuracy = 97.38%: 100%|██████████| 572/572 [00:17<00:00, 32.14it/s]\n",
            "[5 / 10]   Val: Loss = 25.93545, Accuracy = 94.41%: 100%|██████████| 13/13 [00:00<00:00, 31.83it/s]\n",
            "[6 / 10] Train: Loss = 2.40449, Accuracy = 98.07%: 100%|██████████| 572/572 [00:16<00:00, 33.90it/s]\n",
            "[6 / 10]   Val: Loss = 25.06215, Accuracy = 94.74%: 100%|██████████| 13/13 [00:00<00:00, 32.24it/s]\n",
            "[7 / 10] Train: Loss = 1.70320, Accuracy = 98.51%: 100%|██████████| 572/572 [00:17<00:00, 32.65it/s]\n",
            "[7 / 10]   Val: Loss = 25.59882, Accuracy = 94.87%: 100%|██████████| 13/13 [00:00<00:00, 33.56it/s]\n",
            "[8 / 10] Train: Loss = 1.40672, Accuracy = 98.71%: 100%|██████████| 572/572 [00:17<00:00, 30.45it/s]\n",
            "[8 / 10]   Val: Loss = 25.80422, Accuracy = 94.90%: 100%|██████████| 13/13 [00:00<00:00, 32.19it/s]\n",
            "[9 / 10] Train: Loss = 1.35193, Accuracy = 98.76%: 100%|██████████| 572/572 [00:17<00:00, 33.33it/s]\n",
            "[9 / 10]   Val: Loss = 28.37068, Accuracy = 95.01%: 100%|██████████| 13/13 [00:00<00:00, 32.20it/s]\n",
            "[10 / 10] Train: Loss = 1.02519, Accuracy = 98.97%: 100%|██████████| 572/572 [00:18<00:00, 30.90it/s]\n",
            "[10 / 10]   Val: Loss = 25.20683, Accuracy = 94.87%: 100%|██████████| 13/13 [00:00<00:00, 34.31it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfIO4vvi_u3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbe56635-7434-4820-9afe-ff40c79525e3"
      },
      "source": [
        "accuracy =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {accuracy * 100} %')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 94.83318357552785 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMCfmZXtAVjt",
        "colab_type": "text"
      },
      "source": [
        "#Предобученные эмбеддинги"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgepFW3iAS0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3bcd0860-8be2-427e-a18c-bf0bf857d196"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kD8aMDJAyb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd7df054-699a-42e6-bf1e-38d800e6da46"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LuB9khwA2Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], \n",
        "                             lstm_hidden_dim, \n",
        "                             lstm_layers_count, \n",
        "                             bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcTYPIXFBZe1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6333d94c-bea5-41ec-d232-e7bfad30fb2e"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    lstm_layers_count=2,\n",
        "    embeddings=torch.FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "model.type(torch.cuda.FloatTensor)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).type(torch.cuda.FloatTensor)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 10] Train: Loss = 43.73999, Accuracy = 78.78%: 100%|██████████| 572/572 [00:15<00:00, 35.84it/s]\n",
            "[1 / 10]   Val: Loss = 30.65495, Accuracy = 90.05%: 100%|██████████| 13/13 [00:00<00:00, 43.64it/s]\n",
            "[2 / 10] Train: Loss = 15.10263, Accuracy = 92.09%: 100%|██████████| 572/572 [00:15<00:00, 37.66it/s]\n",
            "[2 / 10]   Val: Loss = 20.04592, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 53.97it/s]\n",
            "[3 / 10] Train: Loss = 11.36686, Accuracy = 93.85%: 100%|██████████| 572/572 [00:15<00:00, 36.61it/s]\n",
            "[3 / 10]   Val: Loss = 17.30680, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 43.06it/s]\n",
            "[4 / 10] Train: Loss = 9.46754, Accuracy = 94.78%: 100%|██████████| 572/572 [00:15<00:00, 40.24it/s]\n",
            "[4 / 10]   Val: Loss = 15.63418, Accuracy = 94.87%: 100%|██████████| 13/13 [00:00<00:00, 48.74it/s]\n",
            "[5 / 10] Train: Loss = 8.36105, Accuracy = 95.22%: 100%|██████████| 572/572 [00:15<00:00, 37.35it/s]\n",
            "[5 / 10]   Val: Loss = 14.70704, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 50.42it/s]\n",
            "[6 / 10] Train: Loss = 7.69244, Accuracy = 95.55%: 100%|██████████| 572/572 [00:15<00:00, 36.95it/s]\n",
            "[6 / 10]   Val: Loss = 13.91792, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 45.85it/s]\n",
            "[7 / 10] Train: Loss = 7.19840, Accuracy = 95.77%: 100%|██████████| 572/572 [00:15<00:00, 36.58it/s]\n",
            "[7 / 10]   Val: Loss = 12.52138, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 44.66it/s]\n",
            "[8 / 10] Train: Loss = 6.65670, Accuracy = 95.99%: 100%|██████████| 572/572 [00:15<00:00, 37.73it/s]\n",
            "[8 / 10]   Val: Loss = 13.20143, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 44.96it/s]\n",
            "[9 / 10] Train: Loss = 6.32202, Accuracy = 96.09%: 100%|██████████| 572/572 [00:15<00:00, 38.05it/s]\n",
            "[9 / 10]   Val: Loss = 12.25369, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 50.79it/s]\n",
            "[10 / 10] Train: Loss = 6.17363, Accuracy = 96.20%: 100%|██████████| 572/572 [00:15<00:00, 37.85it/s]\n",
            "[10 / 10]   Val: Loss = 12.05808, Accuracy = 95.84%: 100%|██████████| 13/13 [00:00<00:00, 45.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6x945P-Bwv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c7d8855-5ee4-446b-d9de-92c19d3b51d7"
      },
      "source": [
        "accuracy =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {accuracy * 100} %')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 95.62569499671221 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUo5PVTSCEEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}