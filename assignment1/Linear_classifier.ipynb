{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"../data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "[[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n",
      "[[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "[[ 0.44039854 -0.44039854]\n",
      " [ 0.02371294 -0.02371294]]\n",
      "[[ 0.44039854  0.02371294]\n",
      " [-0.44039854 -0.02371294]]\n",
      "[[[-4.40398539e-01 -8.80797078e-01 -5.55111512e-17]\n",
      "  [-4.40398539e-01 -8.80797078e-01 -5.55111512e-17]\n",
      "  [-4.40398539e-01 -8.80797078e-01 -5.55111512e-17]]\n",
      "\n",
      " [[-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]]]\n",
      "--------------------\n",
      "--------------------\n",
      "[[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n",
      "[[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "[[ 0.44039854 -0.44039854]\n",
      " [ 0.02371294 -0.02371294]]\n",
      "[[ 0.44039854  0.02371294]\n",
      " [-0.44039854 -0.02371294]]\n",
      "[[[-4.40398539e-01 -8.80797078e-01 -5.55111512e-17]\n",
      "  [-4.40398539e-01 -8.80797078e-01 -5.55111512e-17]\n",
      "  [-4.40398539e-01 -8.80797078e-01 -5.55111512e-17]]\n",
      "\n",
      " [[-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]]]\n",
      "--------------------\n",
      "--------------------\n",
      "[[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n",
      "[[ 1.00001  2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "[[ 0.44039801 -0.44039801]\n",
      " [ 0.02371294 -0.02371294]]\n",
      "[[ 0.44039801  0.02371294]\n",
      " [-0.44039801 -0.02371294]]\n",
      "[[[-4.40398014e-01 -8.80796028e-01  0.00000000e+00]\n",
      "  [-4.40398014e-01 -8.80796028e-01  0.00000000e+00]\n",
      "  [-4.40398014e-01 -8.80796028e-01  0.00000000e+00]]\n",
      "\n",
      " [[-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]]]\n",
      "--------------------\n",
      "--------------------\n",
      "[[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n",
      "[[ 0.99999  2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "[[ 0.44039906 -0.44039906]\n",
      " [ 0.02371294 -0.02371294]]\n",
      "[[ 0.44039906  0.02371294]\n",
      " [-0.44039906 -0.02371294]]\n",
      "[[[-4.40399064e-01 -8.80798128e-01  5.55111512e-17]\n",
      "  [-4.40399064e-01 -8.80798128e-01  5.55111512e-17]\n",
      "  [-4.40399064e-01 -8.80798128e-01  5.55111512e-17]]\n",
      "\n",
      " [[-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]]]\n",
      "--------------------\n",
      "--------------------\n",
      "[[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n",
      "[[ 1.       2.00001]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "[[ 0.44039906 -0.44039906]\n",
      " [ 0.02371294 -0.02371294]]\n",
      "[[ 0.44039906  0.02371294]\n",
      " [-0.44039906 -0.02371294]]\n",
      "[[[-4.40399064e-01 -8.80798128e-01  5.55111512e-17]\n",
      "  [-4.40399064e-01 -8.80798128e-01  5.55111512e-17]\n",
      "  [-4.40399064e-01 -8.80798128e-01  5.55111512e-17]]\n",
      "\n",
      " [[-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]]]\n",
      "--------------------\n",
      "--------------------\n",
      "[[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n",
      "[[ 1.       1.99999]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "[[ 0.44039801 -0.44039801]\n",
      " [ 0.02371294 -0.02371294]]\n",
      "[[ 0.44039801  0.02371294]\n",
      " [-0.44039801 -0.02371294]]\n",
      "[[[-4.40398014e-01 -8.80796028e-01  0.00000000e+00]\n",
      "  [-4.40398014e-01 -8.80796028e-01  0.00000000e+00]\n",
      "  [-4.40398014e-01 -8.80796028e-01  0.00000000e+00]]\n",
      "\n",
      " [[-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]\n",
      "  [-2.37129366e-02 -4.74258732e-02  7.63278329e-17]]]\n",
      "--------------------\n",
      "Gradients are different at (0, 1). Analytic: -0.02371, Numeric: 0.44040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ 0.44039854 -0.44039854]\n",
    " [ 0.02371294 -0.02371294]]\n",
    "\n",
    "-0.4403985389922482 0.4403985389922482 \n",
    "-0.4166856024112597 0.4166856024112597 \n",
    "0.4641114755732367 -0.4641114755732367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29, 40, 51],\n",
       "       [39, 54, 69]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = [[1,2,3],[4,5,6]]\n",
    "x2 = [[5,6],[7,8]]\n",
    "np.dot(x2,x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "(3, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 1],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[2, 3],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[4, 5],\n",
       "        [4, 5]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.arange(6).reshape(3,2)\n",
    "print(xx)\n",
    "\n",
    "yy = np.array([[0,0],[1,1],[2,2]])\n",
    "print(xx[yy].shape)\n",
    "# shape[0] - len(classes)\n",
    "# shape[1] - len(features)\n",
    "# shape[2] - len(samples)\n",
    "xx[yy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 1]\n",
      " [2 2]]\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 1],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[2, 3],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[4, 5],\n",
       "        [4, 5]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_classes=3\n",
    "len_feature=2\n",
    "#len_samples=2\n",
    "yy = list(range(len_classes))\n",
    "yy = yy * len_feature\n",
    "yy = np.sort(yy).reshape(-1, len_feature)\n",
    "print(yy)\n",
    "\n",
    "xx = np.arange(6).reshape(3,2)\n",
    "print(xx)\n",
    "xx[yy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]]\n",
      "[[0 1]\n",
      " [2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3],\n",
       "       [ 6, 11]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(4).reshape(2,2)\n",
    "print(a)\n",
    "b = np.arange(4).reshape(2,2)\n",
    "print(b)\n",
    "np.tensordot(a,b, axes=([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
